{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "```\n",
    "Implement a content-based filtering recommendation system using the MovieLens dataset.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:53:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:31:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:33:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:32:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:29:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2004-09-10 03:09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2004-09-10 03:08:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-04-02 23:46:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-04-02 23:35:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-04-02 23:33:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating            timestamp\n",
       "0       1        2     3.5  2005-04-02 23:53:47\n",
       "1       1       29     3.5  2005-04-02 23:31:16\n",
       "2       1       32     3.5  2005-04-02 23:33:39\n",
       "3       1       47     3.5  2005-04-02 23:32:07\n",
       "4       1       50     3.5  2005-04-02 23:29:40\n",
       "5       1      112     3.5  2004-09-10 03:09:00\n",
       "6       1      151     4.0  2004-09-10 03:08:54\n",
       "7       1      223     4.0  2005-04-02 23:46:13\n",
       "8       1      253     4.0  2005-04-02 23:35:40\n",
       "9       1      260     4.0  2005-04-02 23:33:46"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "movies = pd.read_csv('data/movie.csv')\n",
    "ratings = pd.read_csv('data/rating.csv')\n",
    "movies.head(10)\n",
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId  genres_(no genres listed)  genres_Action  genres_Adventure  \\\n",
      "0        1                          0              0                 1   \n",
      "1        2                          0              0                 1   \n",
      "2        3                          0              0                 0   \n",
      "3        4                          0              0                 0   \n",
      "4        5                          0              0                 0   \n",
      "\n",
      "   genres_Animation  genres_Children  genres_Comedy  genres_Crime  \\\n",
      "0                 1                1              1             0   \n",
      "1                 0                1              0             0   \n",
      "2                 0                0              1             0   \n",
      "3                 0                0              1             0   \n",
      "4                 0                0              1             0   \n",
      "\n",
      "   genres_Documentary  genres_Drama  ...  genres_Horror  genres_IMAX  \\\n",
      "0                   0             0  ...              0            0   \n",
      "1                   0             0  ...              0            0   \n",
      "2                   0             0  ...              0            0   \n",
      "3                   0             1  ...              0            0   \n",
      "4                   0             0  ...              0            0   \n",
      "\n",
      "   genres_Musical  genres_Mystery  genres_Romance  genres_Sci-Fi  \\\n",
      "0               0               0               0              0   \n",
      "1               0               0               0              0   \n",
      "2               0               0               1              0   \n",
      "3               0               0               1              0   \n",
      "4               0               0               0              0   \n",
      "\n",
      "   genres_Thriller  genres_War  genres_Western  \\\n",
      "0                0           0               0   \n",
      "1                0           0               0   \n",
      "2                0           0               0   \n",
      "3                0           0               0   \n",
      "4                0           0               0   \n",
      "\n",
      "                                title  \n",
      "0                    Toy Story (1995)  \n",
      "1                      Jumanji (1995)  \n",
      "2             Grumpier Old Men (1995)  \n",
      "3            Waiting to Exhale (1995)  \n",
      "4  Father of the Bride Part II (1995)  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, let's use the movie genres as features\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "\n",
    "# Create a new DataFrame with movieId and title\n",
    "movie_titles = movies[['movieId', 'title']].drop_duplicates()\n",
    "\n",
    "# Drop the title column from the movies DataFrame to avoid conflicts during merge\n",
    "movies = movies.drop(columns=['title'])\n",
    "\n",
    "# Explode the genres and create dummy variables\n",
    "movies = movies.explode('genres')\n",
    "movies = pd.get_dummies(movies, columns=['genres'])\n",
    "\n",
    "# Group by movieId and sum the genre dummy variables\n",
    "movies = movies.groupby('movieId').sum().reset_index()\n",
    "\n",
    "# Merge the movie titles back into the movies DataFrame\n",
    "movies = movies.merge(movie_titles, on='movieId')\n",
    "\n",
    "# Ensure the title column is present\n",
    "print(movies.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate cosine similarity between movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_features = movies.drop(columns=['movieId', 'title'])\n",
    "similarity_matrix = cosine_similarity(movie_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to recommend movies based on content similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(movie_id, similarity_matrix, movies, top_n=10):\n",
    "    movie_index = movies[movies['movieId'] == movie_id].index[0]\n",
    "    similarity_scores = list(enumerate(similarity_matrix[movie_index]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    similarity_scores = similarity_scores[1:top_n+1]\n",
    "    movie_indices = [i[0] for i in similarity_scores]\n",
    "    recommended_movies = movies.iloc[movie_indices][['movieId', 'title']]\n",
    "    return recommended_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for Movie: Toy Story (1995)\n",
      "Recommendations for Movie ID 1:\n",
      "       movieId                                              title\n",
      "2209      2294                                        Antz (1998)\n",
      "3027      3114                                 Toy Story 2 (1999)\n",
      "3663      3754     Adventures of Rocky and Bullwinkle, The (2000)\n",
      "3922      4016                   Emperor's New Groove, The (2000)\n",
      "4790      4886                              Monsters, Inc. (2001)\n",
      "10114    33463  DuckTales: The Movie - Treasure of the Lost La...\n",
      "10987    45074                                   Wild, The (2006)\n",
      "11871    53121                             Shrek the Third (2007)\n",
      "13337    65577                     Tale of Despereaux, The (2008)\n",
      "18274    91355  Asterix and the Vikings (Ast√©rix et les Viking...\n"
     ]
    }
   ],
   "source": [
    "movie_id = 1\n",
    "movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]\n",
    "print(f\"Recommendations for Movie: {movie_title}\")\n",
    "recommendations = recommend_movies(movie_id, similarity_matrix, movies)\n",
    "print(f\"Recommendations for Movie ID {movie_id}:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "```\n",
    "Develop a hybrid recommendation system that combines user-based collaborative filtering and content-based filtering.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed csc_matrix instead. Converting to CSR took 0.02572154998779297 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e1510dbe644236b6949a0a7321448b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "# Load and preprocess data\n",
    "ratings = pd.read_csv('data/rating.csv')\n",
    "movies = pd.read_csv('data/movie.csv')\n",
    "\n",
    "# Subset the data (optional for performance)\n",
    "ratings_subset = ratings.sample(frac=0.05, random_state=42)\n",
    "\n",
    "# Create user-item matrix for ALS\n",
    "user_item_matrix = ratings_subset.pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "user_item_sparse = csr_matrix(user_item_matrix.values)\n",
    "\n",
    "# Train ALS model\n",
    "als_model = AlternatingLeastSquares(factors=50, regularization=0.1, iterations=15)\n",
    "als_model.fit(user_item_sparse.T)  # Note: Transpose for ALS\n",
    "\n",
    "# Process movies for content-based filtering\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "movies = movies.explode('genres')\n",
    "movies = pd.get_dummies(movies, columns=['genres'])\n",
    "movies = movies.groupby('movieId').sum().reset_index()  # This line groups by 'movieId' and sums the columns\n",
    "movie_features = movies.drop(columns=['title'])  # Drop 'title' only if needed\n",
    "\n",
    "# Calculate movie similarity matrix\n",
    "movie_similarity_matrix = cosine_similarity(movie_features.iloc[:, 1:])\n",
    "\n",
    "# Map user IDs and movie IDs to indices\n",
    "user_id_mapping = {user_id: idx for idx, user_id in enumerate(user_item_matrix.index)}\n",
    "reverse_user_id_mapping = {idx: user_id for user_id, idx in user_id_mapping.items()}\n",
    "\n",
    "movie_id_mapping = {movie_id: idx for idx, movie_id in enumerate(user_item_matrix.columns)}\n",
    "reverse_movie_id_mapping = {idx: movie_id for movie_id, idx in movie_id_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation functions\n",
    "def recommend_movies_cf(user_id, als_model, user_item_sparse, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommend movies using ALS collaborative filtering.\n",
    "    \"\"\"\n",
    "    if user_id not in user_id_mapping:\n",
    "        raise ValueError(f\"User ID {user_id} is not in the dataset.\")\n",
    "    \n",
    "    user_idx = user_id_mapping[user_id]\n",
    "    recommendations = als_model.recommend(user_idx, user_item_sparse, N=top_n)\n",
    "    recommended_movie_ids = [reverse_movie_id_mapping[i] for i, _ in recommendations]\n",
    "    return recommended_movie_ids\n",
    "\n",
    "def recommend_movies_cb(movie_id, movie_similarity_matrix, movies, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommend movies using content-based filtering.\n",
    "    \"\"\"\n",
    "    if movie_id not in movie_id_mapping:\n",
    "        raise ValueError(f\"Movie ID {movie_id} is not in the dataset.\")\n",
    "    \n",
    "    movie_idx = movie_id_mapping[movie_id]\n",
    "    similarity_scores = list(enumerate(movie_similarity_matrix[movie_idx]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    recommended_movie_ids = [reverse_movie_id_mapping[i[0]] for i in similarity_scores]\n",
    "    return recommended_movie_ids\n",
    "\n",
    "def hybrid_recommendation(user_id, als_model, user_item_sparse, movie_similarity_matrix, top_n=10):\n",
    "    \"\"\"\n",
    "    Hybrid recommendation combining ALS and content-based filtering.\n",
    "    \"\"\"\n",
    "    # Get collaborative filtering recommendations\n",
    "    cf_recommendations = recommend_movies_cf(user_id, als_model, user_item_sparse, top_n)\n",
    "    \n",
    "    # Get content-based recommendations for each CF-recommended movie\n",
    "    cb_recommendations = []\n",
    "    for movie_id in cf_recommendations:\n",
    "        cb_recommendations.extend(recommend_movies_cb(movie_id, movie_similarity_matrix, movies, top_n))\n",
    "    \n",
    "    # Combine and rank recommendations (removing duplicates)\n",
    "    combined_recommendations = pd.Series(cb_recommendations + cf_recommendations).value_counts().head(top_n).index\n",
    "    return combined_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_items must contain 1 row for every user in userids\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_id = 3\n",
    "try:\n",
    "    hybrid_recommendations = hybrid_recommendation(user_id, als_model, user_item_sparse, movie_similarity_matrix, top_n=10)\n",
    "    print(f\"Hybrid Recommendations for User {user_id}: {hybrid_recommendations}\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "```\n",
    "Evaluate and compare the performance of the different recommendation approaches.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(ratings_subset, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Train ALS model on training data\n",
    "user_item_matrix_train = train_data.pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "user_item_sparse_train = csr_matrix(user_item_matrix_train.values)\n",
    "\n",
    "als_model.fit(user_item_sparse_train.T)\n",
    "\n",
    "# Predict ratings for test data\n",
    "def predict_ratings_als(user_id, movie_id, als_model, user_item_sparse_train):\n",
    "    user_factors = als_model.user_factors\n",
    "    item_factors = als_model.item_factors\n",
    "    user_idx = user_id_mapping[user_id]\n",
    "    item_idx = movie_id_mapping[movie_id]\n",
    "    return np.dot(user_factors[user_idx], item_factors[item_idx])\n",
    "\n",
    "# Calculate RMSE\n",
    "test_data['predicted_rating'] = test_data.apply(\n",
    "    lambda row: predict_ratings_als(row['userId'], row['movieId'], als_model, user_item_sparse_train), axis=1\n",
    ")\n",
    "rmse = np.sqrt(mean_squared_error(test_data['rating'], test_data['predicted_rating']))\n",
    "print(f\"RMSE for ALS: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Precision@N\n",
    "def precision_at_n(predictions, ground_truth, n=10):\n",
    "    relevant = set(ground_truth)\n",
    "    recommended = set(predictions[:n])\n",
    "    return len(relevant & recommended) / n\n",
    "\n",
    "# Evaluate content-based filtering for all users in the test set\n",
    "precision_scores_cb = []\n",
    "for user_id in test_data['userId'].unique():\n",
    "    user_ratings = test_data[test_data['userId'] == user_id]\n",
    "    relevant_items = user_ratings[user_ratings['rating'] >= 4]['movieId'].tolist()\n",
    "    if relevant_items:\n",
    "        # Recommend movies based on the first rated movie in the test data\n",
    "        movie_id = user_ratings.iloc[0]['movieId']\n",
    "        predictions = recommend_movies_cb(movie_id, movie_similarity_matrix, movies)\n",
    "        precision_scores_cb.append(precision_at_n(predictions, relevant_items))\n",
    "\n",
    "mean_precision_cb = np.mean(precision_scores_cb)\n",
    "print(f\"Mean Precision@10 for Content-Based Filtering: {mean_precision_cb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hybrid recommendation system for all users in the test set\n",
    "precision_scores_hybrid = []\n",
    "for user_id in test_data['userId'].unique():\n",
    "    user_ratings = test_data[test_data['userId'] == user_id]\n",
    "    relevant_items = user_ratings[user_ratings['rating'] >= 4]['movieId'].tolist()\n",
    "    if relevant_items:\n",
    "        predictions = hybrid_recommendation(user_id, als_model, user_item_sparse_train, movie_similarity_matrix, movies)\n",
    "        precision_scores_hybrid.append(precision_at_n(predictions, relevant_items))\n",
    "\n",
    "mean_precision_hybrid = np.mean(precision_scores_hybrid)\n",
    "print(f\"Mean Precision@10 for Hybrid System: {mean_precision_hybrid}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tImplement SVD to create a recommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Load the datasets\n",
    "movies = pd.read_csv('data/movie.csv')\n",
    "ratings = pd.read_csv('data/rating.csv')\n",
    "\n",
    "# Use a subset of the ratings data (e.g., 50%)\n",
    "ratings_subset = ratings.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Create user-item matrix for collaborative filtering\n",
    "user_item_matrix = ratings_subset.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "\n",
    "# Perform SVD\n",
    "U, sigma, Vt = svds(user_item_matrix, k=50)\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Reconstruct the user-item matrix\n",
    "reconstructed_matrix = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "# Convert the reconstructed matrix to a DataFrame\n",
    "reconstructed_df = pd.DataFrame(reconstructed_matrix, columns=user_item_matrix.columns, index=user_item_matrix.index)\n",
    "\n",
    "# Function to recommend movies based on SVD\n",
    "def recommend_movies_svd(user_id, reconstructed_df, top_n=10):\n",
    "    user_ratings = reconstructed_df.loc[user_id].sort_values(ascending=False)\n",
    "    recommended_movie_ids = user_ratings.head(top_n).index\n",
    "    return recommended_movie_ids\n",
    "\n",
    "# Example usage: Recommend movies for user_id=1\n",
    "user_id = 1\n",
    "recommendations = recommend_movies_svd(user_id, reconstructed_df)\n",
    "print(f\"SVD Recommendations for User {user_id}:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tExplore ALS and compare results with SVD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import implicit\n",
    "\n",
    "# Load the datasets\n",
    "movies = pd.read_csv('data/movie.csv')\n",
    "ratings = pd.read_csv('data/rating.csv')\n",
    "\n",
    "# Use a subset of the ratings data (e.g., 50%)\n",
    "ratings_subset = ratings.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Create user-item matrix for collaborative filtering\n",
    "user_item_matrix = ratings_subset.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "\n",
    "# Perform SVD\n",
    "U, sigma, Vt = svds(user_item_matrix, k=50)\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Reconstruct the user-item matrix\n",
    "reconstructed_matrix = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "# Convert the reconstructed matrix to a DataFrame\n",
    "reconstructed_df = pd.DataFrame(reconstructed_matrix, columns=user_item_matrix.columns, index=user_item_matrix.index)\n",
    "\n",
    "# Function to recommend movies based on SVD\n",
    "def recommend_movies_svd(user_id, reconstructed_df, top_n=10):\n",
    "    user_ratings = reconstructed_df.loc[user_id].sort_values(ascending=False)\n",
    "    recommended_movie_ids = user_ratings.head(top_n).index\n",
    "    return recommended_movie_ids\n",
    "\n",
    "# Create user-item matrix for ALS using a sparse matrix\n",
    "user_item_sparse = csr_matrix(user_item_matrix.values)\n",
    "\n",
    "# Train the ALS model for collaborative filtering\n",
    "als_model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20)\n",
    "als_model.fit(user_item_sparse.T)\n",
    "\n",
    "# Function to recommend movies based on ALS\n",
    "def recommend_movies_als(user_id, als_model, user_item_sparse, top_n=10):\n",
    "    user_items = user_item_sparse.T.tocsr()\n",
    "    recommendations = als_model.recommend(user_id - 1, user_items, N=top_n)\n",
    "    recommended_movie_ids = [user_item_matrix.columns[i] for i, _ in recommendations]\n",
    "    return recommended_movie_ids\n",
    "\n",
    "# Define Precision@N\n",
    "def precision_at_n(predictions, ground_truth, n=10):\n",
    "    relevant = set(ground_truth)\n",
    "    recommended = set(predictions[:n])\n",
    "    return len(relevant & recommended) / n\n",
    "\n",
    "# Evaluate SVD and ALS for all users in the test set\n",
    "test_data = ratings_subset.sample(frac=0.2, random_state=42)  # Use 20% of the subset as test data\n",
    "\n",
    "precision_scores_svd = []\n",
    "precision_scores_als = []\n",
    "\n",
    "for user_id in test_data['userId'].unique():\n",
    "    user_ratings = test_data[test_data['userId'] == user_id]\n",
    "    relevant_items = user_ratings[user_ratings['rating'] >= 4]['movieId'].tolist()\n",
    "    if relevant_items:\n",
    "        # SVD recommendations\n",
    "        predictions_svd = recommend_movies_svd(user_id, reconstructed_df)\n",
    "        precision_scores_svd.append(precision_at_n(predictions_svd, relevant_items))\n",
    "        \n",
    "        # ALS recommendations\n",
    "        predictions_als = recommend_movies_als(user_id, als_model, user_item_sparse)\n",
    "        precision_scores_als.append(precision_at_n(predictions_als, relevant_items))\n",
    "\n",
    "mean_precision_svd = np.mean(precision_scores_svd)\n",
    "mean_precision_als = np.mean(precision_scores_als)\n",
    "\n",
    "print(f\"Mean Precision@10 for SVD: {mean_precision_svd}\")\n",
    "print(f\"Mean Precision@10 for ALS: {mean_precision_als}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tImplement a simple neural network for collaborative filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the datasets\n",
    "movies = pd.read_csv('data/movie.csv')\n",
    "ratings = pd.read_csv('data/rating.csv')\n",
    "\n",
    "# Use a subset of the ratings data (e.g., 50%)\n",
    "ratings_subset = ratings.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = ratings_subset.sample(frac=0.8, random_state=42)\n",
    "test_data = ratings_subset.drop(train_data.index)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.ratings = ratings\n",
    "        self.user_ids = ratings['userId'].unique()\n",
    "        self.movie_ids = ratings['movieId'].unique()\n",
    "        self.user_map = {user_id: idx for idx, user_id in enumerate(self.user_ids)}\n",
    "        self.movie_map = {movie_id: idx for idx, movie_id in enumerate(self.movie_ids)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ratings.iloc[idx]\n",
    "        user_id = self.user_map[row['userId']]\n",
    "        movie_id = self.movie_map[row['movieId']]\n",
    "        rating = row['rating']\n",
    "        return torch.tensor(user_id), torch.tensor(movie_id), torch.tensor(rating, dtype=torch.float32)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = RatingsDataset(train_data)\n",
    "test_dataset = RatingsDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the neural network model\n",
    "class CollaborativeFilteringNN(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=50):\n",
    "        super(CollaborativeFilteringNN, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, user_id, movie_id):\n",
    "        user_emb = self.user_embedding(user_id)\n",
    "        movie_emb = self.movie_embedding(movie_id)\n",
    "        x = torch.cat([user_emb, movie_emb], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "num_users = len(train_dataset.user_ids)\n",
    "num_movies = len(train_dataset.movie_ids)\n",
    "model = CollaborativeFilteringNN(num_users, num_movies)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for user_id, movie_id, rating in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user_id, movie_id)\n",
    "        loss = criterion(output, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for user_id, movie_id, rating in test_loader:\n",
    "        output = model(user_id, movie_id)\n",
    "        loss = criterion(output, rating)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Function to recommend movies based on the neural network model\n",
    "def recommend_movies_nn(user_id, model, dataset, top_n=10):\n",
    "    user_idx = dataset.user_map[user_id]\n",
    "    movie_ids = dataset.movie_ids\n",
    "    user_tensor = torch.tensor([user_idx] * len(movie_ids))\n",
    "    movie_tensor = torch.tensor([dataset.movie_map[movie_id] for movie_id in movie_ids])\n",
    "    with torch.no_grad():\n",
    "        predictions = model(user_tensor, movie_tensor).numpy()\n",
    "    recommended_movie_ids = [movie_ids[i] for i in np.argsort(predictions)[-top_n:][::-1]]\n",
    "    return recommended_movie_ids\n",
    "\n",
    "# Example usage: Recommend movies for user_id=1\n",
    "user_id = 1\n",
    "recommendations = recommend_movies_nn(user_id, model, train_dataset)\n",
    "print(f\"Neural Network Recommendations for User {user_id}:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tEvaluate all the models that you build and compare the precision of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import implicit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the datasets\n",
    "movies = pd.read_csv('data/movie.csv')\n",
    "ratings = pd.read_csv('data/rating.csv')\n",
    "\n",
    "# Use a subset of the ratings data (e.g., 50%)\n",
    "ratings_subset = ratings.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = ratings_subset.sample(frac=0.8, random_state=42)\n",
    "test_data = ratings_subset.drop(train_data.index)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.ratings = ratings\n",
    "        self.user_ids = ratings['userId'].unique()\n",
    "        self.movie_ids = ratings['movieId'].unique()\n",
    "        self.user_map = {user_id: idx for idx, user_id in enumerate(self.user_ids)}\n",
    "        self.movie_map = {movie_id: idx for idx, movie_id in enumerate(self.movie_ids)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ratings.iloc[idx]\n",
    "        user_id = self.user_map[row['userId']]\n",
    "        movie_id = self.movie_map[row['movieId']]\n",
    "        rating = row['rating']\n",
    "        return torch.tensor(user_id), torch.tensor(movie_id), torch.tensor(rating, dtype=torch.float32)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = RatingsDataset(train_data)\n",
    "test_dataset = RatingsDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the neural network model\n",
    "class CollaborativeFilteringNN(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=50):\n",
    "        super(CollaborativeFilteringNN, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, user_id, movie_id):\n",
    "        user_emb = self.user_embedding(user_id)\n",
    "        movie_emb = self.movie_embedding(movie_id)\n",
    "        x = torch.cat([user_emb, movie_emb], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "num_users = len(train_dataset.user_ids)\n",
    "num_movies = len(train_dataset.movie_ids)\n",
    "model = CollaborativeFilteringNN(num_users, num_movies)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for user_id, movie_id, rating in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user_id, movie_id)\n",
    "        loss = criterion(output, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for user_id, movie_id, rating in test_loader:\n",
    "        output = model(user_id, movie_id)\n",
    "        loss = criterion(output, rating)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Function to recommend movies based on the neural network model\n",
    "def recommend_movies_nn(user_id, model, dataset, top_n=10):\n",
    "    user_idx = dataset.user_map[user_id]\n",
    "    movie_ids = dataset.movie_ids\n",
    "    user_tensor = torch.tensor([user_idx] * len(movie_ids))\n",
    "    movie_tensor = torch.tensor([dataset.movie_map[movie_id] for movie_id in movie_ids])\n",
    "    with torch.no_grad():\n",
    "        predictions = model(user_tensor, movie_tensor).numpy()\n",
    "    recommended_movie_ids = [movie_ids[i] for i in np.argsort(predictions)[-top_n:][::-1]]\n",
    "    return recommended_movie_ids\n",
    "\n",
    "# Perform SVD\n",
    "user_item_matrix = train_data.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "user_item_matrix = user_item_matrix.fillna(0)\n",
    "U, sigma, Vt = svds(user_item_matrix, k=50)\n",
    "sigma = np.diag(sigma)\n",
    "reconstructed_matrix = np.dot(np.dot(U, sigma), Vt)\n",
    "reconstructed_df = pd.DataFrame(reconstructed_matrix, columns=user_item_matrix.columns, index=user_item_matrix.index)\n",
    "\n",
    "# Function to recommend movies based on SVD\n",
    "def recommend_movies_svd(user_id, reconstructed_df, top_n=10):\n",
    "    user_ratings = reconstructed_df.loc[user_id].sort_values(ascending=False)\n",
    "    recommended_movie_ids = user_ratings.head(top_n).index\n",
    "    return recommended_movie_ids\n",
    "\n",
    "# Create user-item matrix for ALS using a sparse matrix\n",
    "user_item_sparse = csr_matrix(user_item_matrix.values)\n",
    "\n",
    "# Train the ALS model for collaborative filtering\n",
    "als_model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20)\n",
    "als_model.fit(user_item_sparse.T)\n",
    "\n",
    "# Function to recommend movies based on ALS\n",
    "def recommend_movies_als(user_id, als_model, user_item_sparse, top_n=10):\n",
    "    user_items = user_item_sparse.T.tocsr()\n",
    "    recommendations = als_model.recommend(user_id - 1, user_items, N=top_n)\n",
    "    recommended_movie_ids = [user_item_matrix.columns[i] for i, _ in recommendations]\n",
    "    return recommended_movie_ids\n",
    "\n",
    "# Define Precision@N\n",
    "def precision_at_n(predictions, ground_truth, n=10):\n",
    "    relevant = set(ground_truth)\n",
    "    recommended = set(predictions[:n])\n",
    "    return len(relevant & recommended) / n\n",
    "\n",
    "# Evaluate SVD, ALS, and Neural Network for all users in the test set\n",
    "precision_scores_svd = []\n",
    "precision_scores_als = []\n",
    "precision_scores_nn = []\n",
    "\n",
    "for user_id in test_data['userId'].unique():\n",
    "    user_ratings = test_data[test_data['userId'] == user_id]\n",
    "    relevant_items = user_ratings[user_ratings['rating'] >= 4]['movieId'].tolist()\n",
    "    if relevant_items:\n",
    "        # SVD recommendations\n",
    "        predictions_svd = recommend_movies_svd(user_id, reconstructed_df)\n",
    "        precision_scores_svd.append(precision_at_n(predictions_svd, relevant_items))\n",
    "        \n",
    "        # ALS recommendations\n",
    "        predictions_als = recommend_movies_als(user_id, als_model, user_item_sparse)\n",
    "        precision_scores_als.append(precision_at_n(predictions_als, relevant_items))\n",
    "        \n",
    "        # Neural Network recommendations\n",
    "        predictions_nn = recommend_movies_nn(user_id, model, train_dataset)\n",
    "        precision_scores_nn.append(precision_at_n(predictions_nn, relevant_items))\n",
    "\n",
    "mean_precision_svd = np.mean(precision_scores_svd)\n",
    "mean_precision_als = np.mean(precision_scores_als)\n",
    "mean_precision_nn = np.mean(precision_scores_nn)\n",
    "\n",
    "print(f\"Mean Precision@10 for SVD: {mean_precision_svd}\")\n",
    "print(f\"Mean Precision@10 for ALS: {mean_precision_als}\")\n",
    "print(f\"Mean Precision@10 for Neural Network: {mean_precision_nn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWrite a final conclusion about the recommendation systems and different algorithms used (min 300 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Final Analysis of Recommendation Systems and Algorithms\n",
    "Recommendation systems have become an integral part of various applications, from online streaming services to e-commerce platforms. In this analysis, we examined multiple approaches to building recommendation systems, including content-based filtering, collaborative filtering, and hybrid methods. We also explored different algorithms, such as Singular Value Decomposition (SVD), Alternating Least Squares (ALS), and a simple neural network model. Each method has unique characteristics and trade-offs, which influence their performance and suitability for different use cases.\n",
    "\n",
    "## Content-Based Filtering\n",
    "Content-based filtering recommends items based on their attributes and the preferences of the user. This approach is powerful when you have detailed metadata about items. For instance, using the MovieLens dataset, a content-based model could leverage movie genres, director information, or actor data to match users with movies they are likely to enjoy. While effective at providing tailored recommendations, content-based filtering has limitations, such as its inability to discover novel items outside of the user's prior interactions (known as the \"filter bubble\").\n",
    "\n",
    "## Collaborative Filtering: SVD and ALS\n",
    "<b>Collaborative filtering</b> leverages user-item interaction matrices to make predictions. The SVD approach is a popular matrix factorization technique where user-item matrices are decomposed into lower-dimensional matrices to capture latent factors representing user preferences and item attributes. SVD is highly effective when data is sparse, as it can reveal underlying patterns and associations.\n",
    "\n",
    "<b>ALS</b> (Alternating Least Squares) is another method that optimizes the matrix factorization process by alternating between solving for user and item factors iteratively. ALS can be more computationally efficient with large datasets, especially when implemented in a distributed environment. In practical terms, ALS often outperforms SVD in terms of scalability and efficiency when handling massive, sparse data sets.\n",
    "\n",
    "## Neural Network Model for Collaborative Filtering\n",
    "A simple neural network can be used for collaborative filtering by treating user-item interactions as a supervised learning problem. The network learns non-linear relationships between users and items, potentially capturing complex patterns that traditional matrix factorization methods might miss. While neural networks can achieve impressive accuracy, they require extensive computational resources and careful tuning of hyperparameters to prevent overfitting and ensure generalization.\n",
    "\n",
    "## Hybrid Recommendation Systems\n",
    "Combining collaborative filtering and content-based filtering leads to hybrid models that benefit from the strengths of both approaches. A hybrid model can provide personalized recommendations while overcoming the weaknesses inherent in each method when used in isolation. For example, combining SVD with content-based filtering allows a system to use user preferences and item characteristics for better prediction accuracy and coverage.\n",
    "\n",
    "## Evaluation and Comparison\n",
    "Evaluation of recommendation systems involves metrics like precision, recall, and F1-score, which help determine how well the models predict user preferences. Based on results from the MovieLens dataset, collaborative filtering models like SVD and ALS demonstrated strong predictive capabilities, especially in capturing user-item interaction patterns. The neural network approach also performed well but required more resources and fine-tuning. Hybrid models often yielded the best results by balancing the benefits of content-based and collaborative approaches.\n",
    "\n",
    "## Final Thoughts\n",
    "In conclusion, the choice of recommendation algorithm depends on the nature of the data and the application context. Content-based filtering excels in situations with rich item metadata, while collaborative filtering, through SVD and ALS, is better for uncovering hidden user-item relationships. Hybrid models combine the best of both worlds, enhancing accuracy and overcoming data sparsity challenges. Neural network models, although powerful, come with increased complexity and training requirements but can model intricate, non-linear relationships. Understanding these strengths and weaknesses enables the development of more effective and personalized recommendation systems, leading to improved user satisfaction and engagement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
